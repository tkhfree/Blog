* * *

title: 网络平台开发套件DPDK
date: 2022-11-27 11:54
tags:

- network
- dpdk

categories:

- 技术：知识技能

comments: true
toc: true

* * *
最近在做T4P4s多目标编译器，崔博要求T4P4s的前端编译要与DPDK结合起来实现数据平面的完全可编程，并且数据带宽达到40Gbs。就看一下DPDK这本书，结合白皮书入入门。

在早期的网络收发数据量不是特别大的时候，比如10Mbs、100Mbs的传输速率，数据报文的处理都是经过通用架构的Linux内核态，一个报文到来产生一次中断请求，cpu去处理网络报文，当传输速率提升到万兆，甚至英特尔推出的40Gbs、100Gbs的传输速率标准时，仅靠cpu5Ghz的时钟频率显然是不够看的，因此将网络数据的处理从内核态转到应用态是一个发展方向。再此之前的解决方案有轮询，指的是一次中断批处理一批网络报文，而不是一个报文中断一次；还有内存共享技术，减少处理的数据从内核复制到用户的消耗。

## 数据包处理能力

吞吐、延迟、丢包率、抖动

对于转发，常以包转发率（每秒包转发率pps）而不是bit/s来衡量转发效率

一般所说的接口带宽，1Gbit/s、10Gbit/s、25Gbit/s、40Gbit/s、100Gbit/s，代表以太接口线路上所能承载的最高传输比特率，其单位是 bit/s(bit per second， 位 / 秒)。实际上，不可能每个比特都传输有效数据。以太网每个帧之间会有帧间距(Inter- Packet Gap，IPG)，默认帧间距大小为 12 字节。每个帧还有 7 个字节的前导(Preamble)，和 1 个字节的帧首定界符(Start Frame Delimiter，SFD)。

## Cache和内存

### 系统架构的演进

经典计算机系统中的内存架构：

[![](https://pic.downk.cc/item/5f19765614195aa594489c4e.jpg)](https://pic.downk.cc/item/5f19765614195aa594489c4e.jpg)

所有的数据交换都需要经过北桥

1. 处理器访问内存
2. 处理器访问外设
3. 外设访问处理器
4. 外设访问内存

因此，北桥的性能成了系统性能的瓶颈，因此推出了一种架构：

![](https://pic.downk.cc/item/5f19783614195aa59449ad36.png)

这里增加了内存的访问带宽，缓解了不同设备对同一内存访问的拥塞问题，但是没有改进单一北桥芯片的瓶颈问题。为了解决这个问题，产生了NUMA(non-uniform memory architecture非一致性内存架构)系统

![](https://pic.downk.cc/item/5f1ae97214195aa594168cb0.png)

这样的系统架构不需要一个复杂的北桥就能将内存带宽增加到以前的四倍，但是该系统也有缺点，访问内存所花费的时间与处理器有关，如果处理器访问本地内存时间很短，如果访问远程内存，则需要跨越一个或者两个cpu。因此访问内存的时间也是不确定的。

### Cache系统简介

Cache有三级，以前一级和二级放在处理器内部，三级放在主板上，主要是因为三级cache太大，集成进处理器需要的晶体管数量太多，现在工艺都到个位数纳米了，可以集成更多的晶体管，三级cache也被集成进处理器了。

- 一级cache

  一般分为数据cache和指令cache，数据cache主要用来存储数据，而指令cache用于存放指令。这种cache处理速度最快，一般只需要3-5个指令周期就可以访问到数据，因此成本高，容量小，一般都只有几十kb。在多核处理器内部，每个核心都拥有属于自己的一级cache。

- 二级cache

  数据和指令都无差别的存放在一起。速度比一级cache要慢，处理器大约需要十几个处理周期才能访问到数据，一般几百kb到几mb，在多核处理器内部每个核都拥有自己的二级cache。

- 三级cache

  多核处理器共用一个cache。导致一个问题，有可能一个内核占用了大部分的三级cache，而其他内核用极小一部分cache，从而导致cache不命中，性能下降

#### TLB cache

MMU的一部分

 当程序员直接访问物理地址进行编程时，当程序出现错误，整个系统都崩掉了；或者一个应用程序调用另一个应用程序的写操作，会让另一个应用程序崩掉。因此提出了虚拟内存和分段分页技术。

软件使用虚拟地址访问内存，而处理器负责虚拟地址到物理地址的映射工作，为了完成映射，处理器采取多级页表来进行多次查找找到真正的物理地址。如果查找不到就会产生缺页异常，不会影响到其他的应用程序。

页表存储在内存中，当需要根据虚拟地址查找物理地址时，就要到内存中寻找页目录表和页存表。如果采用TLB Cache的话，集成到处理器内部，就可以大大缩减访问内存的时间，TLB Cache缓存内存中的页表项，采用相连存储器，直接搜索虚拟地址，返回物理地址。如果TLB不命中，则要去内存中查找。

### Cache地址映射和变换

Cache一般只有20-30MB，内存都是以GB为单位的，怎么把内存放到缓存里面呢。这就需要一个映射算法和分块机制。

- 分块机制：是指Cache和内存都是以块进行数据交换，块的大小通常都是以内存一个存储周期中能够访问的数据长度为限。主流块的大小时64字节Bytes，因此一个Cache line就是64字节大小的字节块。
- 映射算法：是指把内存地址空间和Cache地址空间映射起来，把内存中的内容按照某种规则放到Cache中，当处理器需要某个数据块的内容，则可以根据映射规则去Cache中获取

#### 全关联性Cache

全关联型 Cache 是指主存中的任何一块内存都可以映射到 Cache 中的任意一块位置上。在 Cache 中，需要建立一个目录表，目录表的每个表项都有三部分组成:内存块地址、Cache 块号和一个有效位。当处理器需要访问某个内存地址时，首先通过该目录表查询是否该内容缓存在 Cache 中。

首先，用内存的块地址 A 在 Cache 的目录表中进行查询，如果找到等值的内存块地址， 检查有效位是否有效，只有有效的情况下，才能通过 Cache 块号在 Cache 中找到缓存的内 存，并且加上块内地址 B，找到相应数据，这时则称为 Cache 命中，处理器拿到数据返回; 否则称为不命中，处理器则需要在内存中读取相应的数据。

可以看出，使用全关联型 Cache，块的冲突最小(没有冲突)，Cache 的利用率也高，但是需要一个访问速度很快的相联存储器。随着 Cache 容量的增加，其电路设计变得十分复杂，因此只有容量很小的 Cache 才会设计成全关联型的(如一些英特尔处理器中的 TLB Cache)。

#### 直接关联型Cache

直接关联型 Cache 是指主存中的一块内存只能映射到 Cache 的一个特定的块中。假设 一个 Cache 中总共存在 N 个 Cache line，那么内存被分成 N 等分，其中每一等分对应一个 Cache line。举个简单的例子，假设 Cache 的大小是 2K，而一个 Cache line 的大小是 64B， 那么就一共有 2K/64B=32 个 Cache line，那么对应我们的内存，第 1 块(地址 0 ~ 63)，第 33 块(地址 64*32 ~ 64*33-1)，以及第(N*32+1)块(地址 64*(N-1)~ 64*N-1)都被映射到 Cache 第一块中;同理，第 2 块，第 34 块，以及第(N*32+2)块都被映射到 Cache 第二块中;可以依次类推其他内存块。

直接关联型 Cache 的目录表只有两部分组成:区号和有效位。首先，内存地址被分成三部分:区号 A、块号 B 和块内地址 C。根据区号 A 在目录表中找到完全相等的区号，并且在有效位有效的情况下，说明该数据在 Cache 中，然后通过内存地址的块号 B 获得在 Cache 中的块地址，加上块内地址 C，最终找到数据。如果在目录表中 找不到相等的区号，或者有效位无效的情况下，则说明该内容不在 Cache 中，需要到内存中 读取。

可以看出，直接关联是一种很“死”的映射方法，当映射到同一个 Cache 块的多个内存块同时需要缓存在 Cache 中时，只有一个内存块能够缓存，其他块需要被“淘汰”掉。因此，直接关联型命中率是最低的，但是其实现方式最为简单，匹配速度也最快。

#### 组关联型Cache

组关联型 Cache 是目前 Cache 中用的比较广泛的一种方式，是前两种 Cache 的折中形 式。在这种方式下，内存被分为很多组，一个组的大小为多个 Cache line 的大小，一个组映 射到对应的多个连续的 Cache line，也就是一个 Cache 组，并且该组内的任意一块可以映射 到对应 Cache 组的任意一个。可以看出，在组外，其采用直接关联型 Cache 的映射方式，而 在组内，则采用全关联型 Cache 的映射方式。

假设有一个 4 路组关联型 Cache，其大小为 1M，一个 Cache line 的大小为 64B，那么 总共有 16K 个 Cache line，但是在 4 路组关联的情况下，我们并不是简简单单拥有 16K 个 Cache line，而是拥有了 4K 个组，每个组有 4 个 Cache line。一个内存单元可以缓存到它所 对应的组中的任意一个 Cache line 中去。

以 4 路组关联型 Cache 为例介绍其在 Cache 中的查找过程。目录表由三部分组成，分别是“区号 + 块号”、Cache 块号和有效位。当收到一个内存地址时，该地址被分成四部分:区号 A、组号 B、块号 C 和块内地址 D。首先，根据组号 B 按地址查找到一组目录表项，在 4 路组关联中，则有四个表项，每个表项都有可能存放该内存块;然后，根据区号 A 和块号 C 在该组表项中进行关联查找(即并行查找，为了提高效率)，如果匹配且有效位有 效，则表明该数据块缓存在 Cache 中，得到 Cache 块号，加上块内地址 D，可以得到该内存地址在 Cache 中映射的地址，得到数据;如果没有找到匹配项或者有效位无效，则表示该内存块不在 Cache 中，需要处理器到内存中读取。



实际上，直接关联型 Cache 和全关联型 Cache 只是组关联型 Cache 的特殊情况，当组内 Cache Line 数目为 1 时，即为直接关联型 Cache。而当组内 Cache Line 数目和 Cache 大小相 等时，即整个 Cache 只有一个组，这成为全关联型 Cache。

### Cache写策略

内存的数据被加载到 Cache 后，在某个时刻其要被写回内存，对于这个时刻的选取，有如下几个不同的策略。

直写(write-through):所谓直写，就是指在处理器对 Cache 写入的同时，将数据写入到 内存中。这种策略保证了在任何时刻，内存的数据和 Cache 中的数据都是同步的，这种方式 简单、可靠。但由于处理器每次对 Cache 更新时都要对内存进行写操作，因此总线工作繁 忙，内存的带宽被大大占用，因此运行速度会受到影响。假设一段程序在频繁地修改一个局 部变量，尽管这个局部变量的生命周期很短，而且其他进程 / 线程也用不到它，CPU 依然会 频繁地在 Cache 和内存之间交换数据，造成不必要的带宽损失。

回写(write-back):回写相对于直写而言是一种高效的方法。直写不仅浪费时间，而且 有时是不必要的，比如上文提到的局部变量的例子。回写系统通过将 Cache line 的标志位 字段添加一个 Dirty 标志位，当处理器在改写了某个 Cache line 后，并不是马上把其写回内 存，而是将该 Cache line 的 Dirty 标志设置为 1。当处理器再次修改该 Cache line 并且写回到 Cache 中，查表发现该 Dirty 位已经为 1，则先将 Cache line 内容写回到内存中相应的位置， 再将新数据写到 Cache 中。其实，回写策略在多核系统中会引起 Cache 一致性的问题。设想 有两个处理器核心都需要对某个内存块进行读写，其中一个核心已经修改了该数据块，并且 写回到 Cache 中，设置了 Dirty 位;这时另外一个核心也完成了该内存块的修改，并且准备写入到 Cache 中，这时才发现该 Cache line 是“脏”的，在这种情况下，Cache 如何处理呢? 之后的章节我们会继续这个话题。

除了上述这两种写策略，还有 WC(write-combining)和 UC(uncacheable)。这两种策略 都是针对特殊的地址空间来使用的。

write-combining 策略是针对于具体设备内存(如显卡的 RAM)的一种优化处理策略。对 于这些设备来说，数据从 Cache 到内存转移的开销比直接访问相应的内存的开销还要高得 多，所以应该尽量避免过多的数据转移。试想，如果一个 Cache line 里的字被改写了，处理 器将其写回内存，紧接着又一个字被改写了，处理器又将该 Cache line 写回内存，这样就显 得低效，符合这种情况的一个例子就是显示屏上水平相连的像素点数据。write-combining 策 略的引入就是为了解决这种问题，顾名思义，这种策略就是当一个 Cache line 里的数据一个 字一个字地都被改写完了之后，才将该 Cache line 写回到内存中。

uncacheable 内存是一部分特殊的内存，比如 PCI 设备的 I/O 空间通过 MMIO 方式被映射成内存来访问。这种内存是不能缓存在 Cache 中的，因为设备驱动在修改这种内存时，总是期望这种改变能够尽快通过总线写回到设备内部，从而驱动设备做出相应的动作。如果放在 Cache 中，硬件就无法收到指令。

### **Cache** 预取

就是将内存中的数据提前读取到cache中，这样cpu在下个时钟周期就可以直接从cache中读取数据，不用从内存中读取。前面提到的从一级cache读取数据需要几个时钟周期，从二级需要十几个，从三级cache需要几十个，而从内存读取数据需要几百个时钟周期。

### Cache一致性

我们知道三级cache一般是多个cpu共享的。cache里基本单位是cache line，从32字节到64字节、128字节都有。cpu向内存写的流程：cache 从cpu寄存器里取出，通过内部总线发送到内存；读的流程：从内存发给cache，写入到cpu里的寄存器。

那么内存中的数据缓冲区或者数据结构的起始地址与cache line对齐么？

如果没对齐，则一个小于cache line的数据，有可能会占用两个cache line；如果两个核同时对同一个内存区域进行读写，该怎么操作？（多个核对同一个cache line进行操作）

所以定义数据结构或者数据缓冲区时就申明对齐。

DPDK为了减少Cache一致性的问题，通过对每一个核定义单独的数据结构来避免cpu核共享一个数据结构从而写入同一个cache行。

```c
struct lcore_conf{
    ...
} _rte_cache_aligned;//cache line对齐
struct lcore_conf lcore[RTE_MAX_LCORE] _rte_cache_aligned;//定义RTE_MAX_LCORE个数据结构
```

以上定义要求cache line对齐，不会出现一个数据结构横跨两个cache行的问题；对于每个核定义单独的数据结构，防止共享一个结构体，解决了一致性问题。

网卡的多队列问题：

一般网卡都有多队列，接收队列和发送队列都有多个，dpdk中的网卡为每个核都绑定单独的接收队列核发送队列，避免了竞争核cache一致性问题。

### DMA

现代磁盘向内存中拷贝数据时无需借助cpu的帮助，由外接的I/O硬件或者主板进行操作，叫做DMA（Direct Memory Access）

### 零拷贝

直接把磁盘数据copy到了进程空间中，但实际上一般情况下I/O数据是要首先copy到操作系统内部，然后操作系统再copy到进程空间中。**Zero-copy**指无需从内核态向用户态进行拷贝报文，CPU直接在内核态对报文进行操作。

## mbuf结构体

![](https://img.imgdb.cn/item/600e71543ffa7d37b3e97eaf.jpg)

![](https://img.imgdb.cn/item/600e71713ffa7d37b3e994ea.jpg)

有一个知乎的帖子[[译] 写一个简单的内存分配器（替换glibc中的malloc函数）](https://zhuanlan.zhihu.com/p/80648748)讲解为什么设置这种结构体，很有借鉴意义。

![](https://img.imgdb.cn/item/603c5ec35f4313ce25289bd7.png)

## dpdk抓包工具-pdump

1、先设置环境变量RTE_SDK=/home/ndsc/t4p4s/dpdk-19.11/

2、安装依赖libpcap的PMD驱动

​		yum install -y libpcap.x86_64 libpcap-devel.x86_64

3、修改 Target Env 的配置文件

​		`$ vim dpdk-18.08/x86_64-native-linuxapp-gcc/.config CONFIG_RTE_LIBRTE_PMD_PCAP=y  CONFIG_RTE_LIBRTE_PDUMP=y  `

4、安装pdump

​		`cd dpdk-19.11/app/pdump`

​		`make && make install`

5、运行参数

​		` ./build/app/dpdk-pdump -- --pdump 'port=0,queue=*,rx-dev=/tmp/capture.pcap'`

![](https://pic.imgdb.cn/item/60b0474208f74bc1594367fe.jpg)

## DPDK应用示例介绍

这些示例从简单到复杂，但大多数都旨在演示 DPDK 的一个特定功能。下面将重点介绍一些更有趣的示例。

- Hello World(**Hello World**)：与大多数编程框架的介绍一样，一个好的开始是Hello World应用程序。Hello World 示例设置 DPDK 环境抽象层 （EAL），并打印一个简单的"Hello World"消息到每个启用 DPDK 的核心。此应用程序不执行任何数据包转发，但它是测试 DPDK 环境是否编译和正确设置的一个很好的方法。
- 基本转发框架（**Basic Forwarding/Skeleton App**）：基本转发/骨架包含使用DPDK 启用基本数据包转发所需的最小代码量。这允许您测试网络接口是否使用 DPDK。
- 二层转发（**Network Layer 2 Forwarding**）：网络第 2 层转发，或者应用程序确实基于以太网 MAC 地址进行转发，就像一个简单的交换机。`l2fwd`
- 三层转发(**Network Layer 3 Forwarding**)：网络第 3 层转发，或应用程序确实基于互联网协议、IPv4 或 IPv6 进行转发，就像一个简单的路由器。`l3fwd`
- 数据包分发器(**Packet Distributor**)：数据包分发器演示如何将到达 Rx 端口的数据包分发到不同的内核进行处理和传输。
- 多进程应用程序(**Multi-process Application**)：多进程应用程序显示两个 DPDK 进程如何使用队列和内存池来共享信息。
- RX/TX回调应用程序(**RX/TX Callbacks Application**) ：RX/TX回调示例应用程序是一个数据包转发应用程序，演示用户在接收和传输的数据包上使用用户定义的回调。应用程序通过向 RX 和 TX 数据包处理函数添加回调来计算 RX（数据包到达）和 TX（数据包传输）之间的数据包延迟。
- IPSec 安全网关(**IPsec Security Gateway**)：IPSec 安全网关应用程序是更接近真实示例的最小示例。这也是使用 DPDK Cryptodev 框架的应用程序的一个很好的示例。
- 精度时间协议 （**Precision Time Protocol (PTP) Client**)客户端：PTP 客户端是实际应用程序的另一个最小实现。在这种情况下，应用程序是一个 PTP 客户端，它与 PTP 主时钟通信，使用 IEEE1588 协议在网络接口卡 （NIC） 上同步时间。
- 服务质量调度程序 （**QoS Scheduler**） ：QoS 调度程序应用程序演示使用 DPDK 提供 QoS 调度。

## DPDK线程启动方式

### dpdk线程

rte_eal_init执行时会通过pthread_create创建一个worker线程(eal_thread_loop)，并绑定到配置的lcore上，
可通过参数-c/-l/--lcores指定在那些核心创建worker线程。
每个worker线程拥有一对管道fd用于与master线程通信。
worker线程内部时一个while超循环结构，在循环开始处通过read管道阻塞线程。
master线程调用rte_eal_remote_launch或rte_eal_mp_remote_launch创建用户线程时，
实现上是通过write对应core的管道唤醒worker线程，再执行rte_eal_remote_launch传入的函数。

### 几种用户线程初始化方式

在每个slave核上创建线程

```c++
RTE_LCORE_FOREACH_SLAVE(lcore_id) 
	{ rte_eal_remote_launch(user_loop, param, lcore_id); }
```

在除了master核的其他核上创建线程

```c++
rte_eal_mp_remote_launch(user_loop, param, SKIP_MASTER);
```

上面两种dpdk线程初始化方式都会跳过core 0 的0核(默认的master核心)
那么问题来了
对于启用了超线程的cpu，一个物理核上会存在两个两个逻辑核(lcore)
那么data path的任务是否适合放在上面这个cpu的core0-1上呢？
拍脑袋想也应该时不合适的吧？
那么在线程初始化的适合，是否应该根据cpu分布，

1. 避免在master物理核心的两个lcore上跑data path任务
2. 在每个物理核上仅选择一个lcore创建任务

理由是，超线程的两个逻辑核心是共享流水线和L1/L2 Cache的，超线程是否适合dpdk？

不知道为啥，在大部分代码里看到的是上面这两种任务初始化，而不是根据物理核心初始化

### 基于DPDK的接收报文模式有两种物理实现框架：

- RTC（run to complement）是指所有处理都放在一个cpu上进行，从RX到Filter到TX等过程都由一个进程进行处理
- Pipeline流水线模式是由多个进程进行共同处理，RX在一个进程，Filter在另一个进程，等等，通过环形队列的方式不同进程之间进行报文的传递

PPK平台是在RTC模型框架下进行的开发工作，因此需要针对Qos功能进行新的开发（Pipeline有现有的基于DPDk的Qos处理方式）。

程序处理流程：

```c++
void parse_args(int argc, char **argv){
    //lcore_params[nb_lcore_params].port_id = (uint8_t)int_fld[FLD_PORT];
    //lcore_params[nb_lcore_params].queue_id = (uint8_t)int_fld[FLD_QUEUE];
    //lcore_params[nb_lcore_params].lcore_id = (uint8_t)int_fld[FLD_LCORE];
    ret = parse_lcore_params(optarg);
}
void initialize_args(){
    rte_eal_init();//dpdk 环境抽象层的初始化
    parse_args(argc,argv);//解析传入参数，-l -c -n --lcores之类的
}
void main(){
    initialize_args();
    initialize_nic();
}
```

### DPDK的多线程

DPDK通过在多核设备上创建多线程，每个线程绑定到单独的核上。

在initialize_args里调用DPDK多线程的处理机制，针对每一个网卡port，运行一个新线程，进程分为master线程和slave线程

![](https://pic.imgdb.cn/item/62329dcc5baa1a80ab3f100d.jpg)

DPDK线程基于pthread接口创建，属于抢占式线程模型，受内核调度支配。通过在多核设备上创建多个线程，每个线程绑定到单独的核上，减少线程调度的开销， 以提高性能。控制线程一般绑定到MASTER核上，接受用户配置，并传递配置参数给数据线程等；数据线程分布在不同核上处理数据包。

### 对于网卡开启多队列的流程：

1. 启动时进行端口的配置，这部分需要配置的内容包括设置收包的模式，同时指定收包时使用的哈希函数作用的地方，两者缺一不可；
2. 启动多个队列，这个肯定没问题；
3. 利用DPDK自带的统计工具无法显示相关的队列的数据，这个不知道为什么，需要后续进行源码的阅读后做定夺。
    **所以，测试时只需要从多个队列上进行取数据即可，发现不同队列上都有数据。**



作者：VChao
链接：https://www.jianshu.com/p/c6686b022019
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

 -c 0xff -n 8 -- -p 0x3 --config "\"(0,0,0),(0,1,1),(0,2,2),(1,0,3),(1,1,4),(1,2,5)\""

### 网卡的操作

![接收报文流程](https://pic.imgdb.cn/item/623c87c527f86abb2aabf967.jpg)

智能网卡允许DMA操作和多队列。当配置完多队列的参数后，在启动时，调用initialize_nic();函数，将参数配置到网卡上，网卡开启多队列。DMA由一段连续缓存的环形队列和寄存器构成。（这里引申一下DMA技术，DMA打的实现还要依赖于intel的DDIO，即允许DMA引擎直接操作last level cache，这样cpu就可以尽可能少的参与io操作）寄存器包括base,head,tail,lengh。缓存通常采用一段物理上连续的内存。队列中存放描述符，不管是接收队列还是发送队列，主要有这么几个操作：

1. 填充缓冲区地址（mbuf）到描述符；
2. 移动tail尾指针；
3. 判断描述符中的判断位，确定是否收/发完成。

需要注意的是，环形队列的内容在内存中，但是控制部分寄存器是在网卡上的硬件。

关于NIC、cpu、内存（LLC last level cache）三者一起接收报文的流程：

1. cpu将缓存地址放入环形队列中可用的描述符中；
2. DMA引擎读取描述符获取该缓存地址，将报文放入到该地址的内存（缓存）空间；
3. 网卡回写接收侧描述符，改写描述符状态，确认接收完成的前提下；
4. CPU读取接收侧描述符，确认接收完成。
5. CPU去缓存地址读取报文做转发判断；
6. CPU对报文做填充；准备发送走；

接下来进行发送操作：

1. CPU读取发送侧描述符，确定是否发送（上一条报文）完成；
2. CPU将缓冲区地址写入发送侧描述符；
3. 网卡DMA读取发送侧描述符缓冲地址；
4. 去缓冲地址中取报文数据；
5. 网卡写发送侧描述符，确认发送已经完成；

```c++
uint16_t eth_igb_recv_pkts(void *rx_queue, struct rte_mbuf **rx_pkts, uint16_t nb_pkts)
{
	while (nb_rx < nb_pkts) 
	{
		//从描述符队列中找到待被应用层最后一次接收的那个描述符位置
		rxdp = &rx_ring[rx_id];
		staterr = rxdp->wb.upper.status_error;
		//检查状态是否为dd, 不是则说明驱动还没有把报文放到接收队列，直接退出
		if (! (staterr & rte_cpu_to_le_32(E1000_RXD_STAT_DD)))
		{
			break;
		};
		//找到了描述符的位置，也就从软件队列中找到了mbuf
		rxe = &sw_ring[rx_id];
		rx_id++;
		rxm = rxe->mbuf;		
		//填充mbuf
		pkt_len = (uint16_t) (rte_le_to_cpu_16(rxd.wb.upper.length) - rxq->crc_len);
		rxm->data_off = RTE_PKTMBUF_HEADROOM;
		rxm->nb_segs = 1;
		rxm->pkt_len = pkt_len;
		rxm->data_len = pkt_len;
		rxm->port = rxq->port_id;
		rxm->hash.rss = rxd.wb.lower.hi_dword.rss;
		rxm->vlan_tci = rte_le_to_cpu_16(rxd.wb.upper.vlan);
		//保存到应用层
		rx_pkts[nb_rx++] = rxm;
        
        //申请一个新的mbuf
		nmb = rte_rxmbuf_alloc(rxq->mb_pool);
		//因为原来的mbuf被应用层取走了。这里替换原来的软件队列mbuf，这样网卡收到报文后可以放到这个新的mbuf
		rxe->mbuf = nmb;
		dma_addr = rte_cpu_to_le_64(RTE_MBUF_DATlA_DMA_ADDR_DEFAULT(nmb));
		//将mbuf地址保存到描述符中，相当于高速dma控制器mbuf的地址。
		rxdp->read.hdr_addr = dma_addr;			//这里会将dd标记清0
		rxdp->read.pkt_addr = dma_addr;		
	}
}

uint16_t rte_eth_rx_burst(uint8_t port_id,  uint16_t queue_id, struct rte_mbuf **rx_pkts,  uint16_t nb_pkts)
{
	//如果是e1000网卡，则接收报文的接口为eth_igb_recv_pkts
	return (*dev->rx_pkt_burst)(dev->data->rx_queues[queue_id],	rx_pkts, nb_pkts);
}

```

下图是一个大致的函数实现流程

![](https://pic.imgdb.cn/item/62453e2a27f86abb2af06316.jpg)

### Classification功能

Classification功能是指网卡在收包时，将符合某种规则的包放入指定的队列。网卡一般支持一种或多种classification功能，以intel 700系列网卡为例，其支持MAC/VLAN filter、Ethertype filter、Cloud filter、flow director等等。不同的网卡可能支持不同种类的filter，例如intel 82599系列网卡支持n-tuple、L2_tunnel、flow director等，下表列出了DPDK中不同驱动对filter的支持。即使intel 82599系列网卡和intel 700系列网卡都支持flow director，它们支持的方式也不一样。那DPDK是如何支持不同网卡的filter的呢？

![](https://pic.imgdb.cn/item/624661ef27f86abb2ad77379.jpg)

### ring结构

